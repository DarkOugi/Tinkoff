# Над проектом работали
@YouNotKissMe  
@DarkOugi  
Один и тот же человек, но с разных устройств

# Задача для Machine Learning Весна'23
Антиплагиат  
Для решения был использован алгоритм расстояния Левенштейна

## Какие файлы за что отвечают ?

### dist_lev.py - фаил с алгоритмом расчета метрики - расстояние Левинштейна
Сейчас работает с динамически меняющейся матрицей, из-за этого количество 
требуемой памяти уменьшилось с MxN, где M,N - длина первого и второго слова, до N, 
так как создается матрица размером 2xN  
Но сложность алгоритма квадратичная, поэтому обработка больших файлов занимает много времени

### compare.py - файл интерфейса.
Пример использования программы:  
Проверялось на операционной системе Windows, через командную строку cmd  
![изображение с примером запуска скрипта](/pic/img.png)  
Пример данных в input.txt  
![изображение с примером файла, хранящего список файлов к проверки](/pic/img_1.png)  
Пример выходных данных scores.txt  
![изображение с примером файла, хранящего результаты работы алгоритма](/pic/img_2.png) 

Метрика расчета процента схожести двух файлов - 
metric_lev = (origin_file_size - lev_dist)/(origin_file_size)
### files_with_dir.py - добавляет в указанный фаил все файлы в указанных 2 директориях 
Пример использования:    
![изображение с использованием скрипта в командной строке](/pic/img_3.png)  
Получившийся файл  
![изображение с файлом files, в котором хранятся все файлы папок  files plagiat1](pic/img_4.png)
# Итоги
Что не было достигнуто:  
- не удалось применить алгоритмы машинного обучения для DL
Причины:
Очень абстрактное задание, которое требовало создать для начала свои признаки для каждого объекта и целевой признак - процент схожести  
Ход размышления над задчей с использованием мл:  
1 создать 5-6 признаков
например - количество использованных библиотек в документа a/ количество использованных библиотек в документе b
такими же отношениями - функции, классы, переменные и тд  
плюсы такого подхода - при примерно одинаковых файлах, признаки были бы отмасштабированы, в диапозоне от 0 до 1  
далее - самое сложное, нам изначально нужно выдать процент схожести этим файлам, чтобы модел адекватно обучилась  
была идея использовать уже созданные алгоритмы поиска антиплагиата, чтобы они разметили объекты, а далее на них обучить свою модель   
